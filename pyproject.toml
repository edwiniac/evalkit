[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "evalkit"
version = "0.1.0"
description = "Production-grade LLM evaluation framework â€” test hallucination, factuality, relevance, and more"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "Edwin Isac", email = "edwinisac007@gmail.com"}
]
keywords = ["llm", "evaluation", "testing", "ai", "nlp", "hallucination", "rag"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

dependencies = [
    # LLM providers
    "openai>=1.0.0",
    "anthropic>=0.18.0",
    "httpx>=0.27.0",

    # Metrics
    "numpy>=1.24.0",
    "nltk>=3.8.0",
    "rouge-score>=0.1.2",

    # Reporting
    "rich>=13.0.0",
    "jinja2>=3.1.0",

    # Config & CLI
    "pyyaml>=6.0",
    "pydantic>=2.0.0",
    "click>=8.1.0",
]

[project.optional-dependencies]
embeddings = [
    "sentence-transformers>=2.2.0",
]
dev = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.23.0",
    "pytest-cov>=4.1.0",
    "black>=24.0.0",
    "ruff>=0.2.0",
    "mypy>=1.8.0",
]

[project.scripts]
evalkit = "evalkit.cli:main"

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
addopts = "-v --tb=short"
asyncio_mode = "auto"

[tool.black]
line-length = 100
target-version = ['py310']

[tool.ruff]
line-length = 100

[tool.ruff.lint]
select = ["E", "F", "W", "I", "N"]

[tool.ruff.lint.per-file-ignores]
"src/evalkit/metrics/judge_prompts.py" = ["E501"]
"src/evalkit/reporters/html_reporter.py" = ["E501"]
