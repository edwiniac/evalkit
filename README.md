# EvalKit

**Production-grade LLM evaluation framework** â€” test hallucination, factuality, relevance, and more.

Stop vibes-checking your LLMs. EvalKit gives you reproducible, quantitative evaluation with built-in metrics, model comparison, and detailed reporting.

---

## âœ¨ Features

- **ğŸ“ Deterministic Metrics** â€” ExactMatch, ContainsAny/All, RegexMatch, IsJSON, LengthRange
- **ğŸ§  LLM-as-Judge** â€” Faithfulness, Hallucination, Relevance, Coherence, Toxicity
- **ğŸ“Š Statistical Metrics** â€” BLEU, ROUGE, Semantic Similarity
- **âš¡ System Metrics** â€” Latency, Token Count, Cost Estimation
- **ğŸ”„ Model Comparison** â€” Side-by-side evaluation across providers
- **ğŸ“‹ Rich Reporting** â€” Console, JSON, HTML, CSV
- **ğŸ”Œ Pluggable** â€” Custom metrics, any model backend
- **ğŸ§ª Test Suite API** â€” Define, run, and track evaluations like unit tests

## ğŸš€ Quick Start

```python
from evalkit.models import EvalCase
from evalkit.metrics import ExactMatch, ContainsAny
from evalkit.suite import EvalSuite
from evalkit.runners import EvalRunner
from evalkit.reporters import ConsoleReporter

# Define test cases
suite = EvalSuite(
    name="Geography Quiz",
    cases=[
        EvalCase(input="Capital of France?", expected_output="Paris"),
        EvalCase(input="Capital of Japan?", expected_output="Tokyo"),
    ],
    metrics=[ExactMatch(), ContainsAny()],
)

# Run against your model
runner = EvalRunner()
result = await runner.run(suite, your_model_fn, model_name="gpt-4")

# Report results
ConsoleReporter(verbose=True).print(result)
```

## ğŸ“Š Test Coverage

```
90+ tests passing | Sprint-based development

Sprint 1: Foundation (models, deterministic metrics, runner, reporters)
Sprint 2: LLM-as-Judge metrics
Sprint 3: Statistical metrics & model comparison
Sprint 4: CLI & HTML reports
Sprint 5: Polish & examples
```

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CLI / Python API                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Eval Runner                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Metrics  â”‚ Judges   â”‚ Datasets â”‚ Model Adapters â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Reporter Layer                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›  Development

```bash
git clone https://github.com/edwiniac/evalkit.git
cd evalkit
python -m venv venv && source venv/bin/activate
pip install -e ".[dev]"
pytest tests/ -v
```

## ğŸ“ License

MIT â€” Built by Edwin Isac
